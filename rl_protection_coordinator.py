"""
ProtecAI Mini - Sistema de Coordena√ß√£o de Prote√ß√£o com RL
IEEE 14-Bus Protection Coordination System with Reinforcement Learning

Sistema completo de coordena√ß√£o de prote√ß√£o para plataformas offshore
com algoritmo de aprendizado por refor√ßo para otimiza√ß√£o autom√°tica.
"""

import numpy as np
import pandas as pd
from dataclasses import dataclass
from typing import List, Dict, Tuple, Optional
import random
import json
from datetime import datetime

@dataclass
class ProtectionDevice:
    """Dispositivo de prote√ß√£o ANSI com par√¢metros reais"""
    id: str
    zone: str  # Z1 ou Z2
    type: str  # 50/51, 67, 87T, 27/59
    location: str
    pickup_current: float  # em pu
    time_delay: float  # em segundos
    distance_km: float
    coordination_margin: float = 0.3  # IEEE C37.112 padr√£o
    status: str = 'active'

@dataclass
class ProtectionZone:
    """Zona de prote√ß√£o IEEE 14-Bus"""
    id: str
    transformer: str
    power_mva: float
    voltage_kv: float
    buses: List[int]
    devices: List[ProtectionDevice]

@dataclass
class FaultSimulationResult:
    """Resultado completo da simula√ß√£o de falta"""
    fault_location: str
    fault_type: str
    fault_current_a: float
    affected_zone: str
    coordination_ok: bool
    device_responses: List[Dict]
    coordination_issues: List[Dict]
    normative_compliance: Dict

class BasicRLAgent:
    """
    Agente de Aprendizado por Refor√ßo para otimiza√ß√£o de coordena√ß√£o de prote√ß√£o
    Algoritmo: Q-Learning com epsilon-greedy
    """
    
    def __init__(self, state_size=64, action_space_size=32, learning_rate=0.01, epsilon=0.3):
        self.state_size = state_size
        self.action_space_size = action_space_size
        self.learning_rate = learning_rate
        self.epsilon = epsilon
        self.epsilon_decay = 0.99  # Decay mais lento para explora√ß√£o adequada
        self.epsilon_min = 0.05
        
        # Q-table para aprendizado - maior para capturar complexidade
        self.q_table = np.zeros((state_size, action_space_size))
        
        # Hist√≥rico de treinamento
        self.episodes = 0
        self.max_episodes = 1000
        self.training_history = []
        
        # Par√¢metros avan√ßados para coordena√ß√£o de prote√ß√£o
        self.gamma = 0.95  # Fator de desconto
        self.min_exploration_episodes = 50  # M√≠nimo de epis√≥dios para explora√ß√£o
    
    def get_state(self, coordinator, fault_result):
        """
        Estado avan√ßado para coordena√ß√£o de prote√ß√£o baseado em normas t√©cnicas
        Captura: coordena√ß√£o, seletividade, conformidade normativa, tempos, correntes
        """
        state_vector = []
        
        # 1. Estado b√°sico da coordena√ß√£o
        state_vector.append(1 if fault_result.coordination_ok else 0)
        
        # 2. N√∫mero de dispositivos operando (seletividade)
        operating_count = len([d for d in fault_result.device_responses if d['should_operate']])
        state_vector.append(min(operating_count / 8.0, 1.0))  # Normalizado
        
        # 3. Zona afetada
        state_vector.append(1 if fault_result.affected_zone == 'Z1' else 0)
        
        # 4. Tipo de falta (codificado)
        fault_encoding = {'3ph': 0.25, '2ph': 0.5, '1ph': 0.75, '2ph_ground': 1.0}
        state_vector.append(fault_encoding.get(fault_result.fault_type, 0.25))
        
        # 5. Problemas de coordena√ß√£o IEEE C37.112
        coord_issues = len(fault_result.coordination_issues)
        state_vector.append(min(coord_issues / 10.0, 1.0))  # Normalizado
        
        # 6. Conformidade normativa (4 normas)
        for norm, compliance in fault_result.normative_compliance.items():
            state_vector.append(1 if compliance.get('coordination_margins', False) or 
                              compliance.get('goose_performance', False) or 
                              compliance.get('selectivity_dr', False) or 
                              compliance.get('offshore_environment', False) else 0)
        
        # 7. Tempos de opera√ß√£o dos dispositivos prim√°rios (87T, 50/51)
        primary_times = []
        for device in fault_result.device_responses:
            if device['should_operate'] and device['type'] in ['87T', '50/51']:
                primary_times.append(device['operating_time'])
        
        # Estat√≠sticas dos tempos
        if primary_times:
            state_vector.append(min(min(primary_times) / 2.0, 1.0))  # Tempo m√≠nimo
            state_vector.append(min(max(primary_times) / 2.0, 1.0))  # Tempo m√°ximo
            state_vector.append(min(np.std(primary_times) / 1.0, 1.0))  # Desvio padr√£o
        else:
            state_vector.extend([0, 0, 0])
        
        # 8. Corrente de falta normalizada
        state_vector.append(min(fault_result.fault_current_a / 15000.0, 1.0))
        
        # 9. Estado espec√≠fico dos dispositivos cr√≠ticos
        critical_devices = ['87T-TR1', '87T-TR2', '50/51-L4-5', '50/51-L5-6']
        for device_id in critical_devices:
            device_response = next((d for d in fault_result.device_responses if d['device_id'] == device_id), None)
            if device_response:
                state_vector.append(1 if device_response['should_operate'] else 0)
                state_vector.append(min(device_response['operating_time'] / 2.0, 1.0) if device_response['should_operate'] else 0)
            else:
                state_vector.extend([0, 0])
        
        # Preenche at√© state_size com zeros se necess√°rio
        while len(state_vector) < self.state_size:
            state_vector.append(0)
        
        # Trunca se exceder
        state_vector = state_vector[:self.state_size]
        
        # Converte para √≠ndice da Q-table usando hash
        return abs(hash(tuple(np.round(state_vector, 3)))) % self.state_size
    
    def get_action(self, state):
        """Seleciona a√ß√£o usando epsilon-greedy"""
        if np.random.random() <= self.epsilon:
            return np.random.choice(self.action_space_size)
        return np.argmax(self.q_table[state])
    
    def calculate_reward(self, fault_result, critical_load_lost=False, blackout=False):
        """
        Fun√ß√£o de recompensa avan√ßada baseada no documento PARAMETRIZACAO_PROTECAO_BASICO.txt
        Implementa sistema de recompensas/penalidades conforme normas IEEE 242, API RP 14F, ABNT NBR 14039
        """
        reward = 0
        
        # === RECOMPENSAS PRINCIPAIS (conforme documento) ===
        
        # 1. Atua√ß√£o seletiva correta (+10 pontos)
        if fault_result.coordination_ok:
            primary_operated = any(
                d['should_operate'] and d['coordination_ok'] 
                for d in fault_result.device_responses 
                if d['type'] in ['87T', '50/51']
            )
            if primary_operated:
                reward += 10
                
            # Bonifica√ß√£o adicional por coordena√ß√£o perfeita
            if len(fault_result.coordination_issues) == 0:
                reward += 5
        
        # 2. Backup funcionando ap√≥s falha prim√°ria (+5 pontos)
        backup_operated = any(d['should_operate'] and d['type'] == '50/51' for d in fault_result.device_responses)
        differential_failed = not any(d['should_operate'] and d['type'] == '87T' for d in fault_result.device_responses)
        
        if backup_operated and differential_failed:
            reward += 5
        
        # 3. Seletividade adequada (NBR 5410)
        operating_devices = len([d for d in fault_result.device_responses if d['should_operate']])
        if operating_devices <= 2:  # Seletividade excelente
            reward += 8
        elif operating_devices <= 3:  # Seletividade boa
            reward += 4
        elif operating_devices > 5:  # Seletividade ruim
            reward -= 10
        
        # 4. Conformidade normativa (bonifica√ß√µes)
        normative_bonus = 0
        for norm, compliance in fault_result.normative_compliance.items():
            if isinstance(compliance, dict):
                if compliance.get('coordination_margins', False):
                    normative_bonus += 3  # IEEE C37.112
                if compliance.get('goose_performance', False):
                    normative_bonus += 2  # IEC 61850
                if compliance.get('selectivity_dr', False):
                    normative_bonus += 3  # NBR 5410
                if compliance.get('offshore_environment', False):
                    normative_bonus += 2  # API RP 14C
        reward += normative_bonus
        
        # === PENALIDADES (conforme documento) ===
        
        # 5. Problemas de coordena√ß√£o (-5 por problema)
        coordination_issues = len(fault_result.coordination_issues)
        reward -= coordination_issues * 5
        
        # 6. Coordena√ß√£o geral falhando (-25 pontos)
        if not fault_result.coordination_ok:
            reward -= 25
        
        # 7. Carga cr√≠tica desligada (-20 pontos)
        if critical_load_lost:
            reward -= 20
        
        # 8. Blackout geral (-50 pontos)
        if blackout:
            reward -= 50
        
        # 9. Tempos de coordena√ß√£o inadequados
        operating_times = [d['operating_time'] for d in fault_result.device_responses if d['should_operate']]
        if len(operating_times) > 1:
            time_spread = max(operating_times) - min(operating_times)
            if time_spread < 0.2:  # Tempos muito pr√≥ximos (< 200ms)
                reward -= 8
            elif time_spread > 2.0:  # Tempos muito distantes (> 2s)
                reward -= 5
        
        # 10. Dispositivos diferenciais n√£o operando em falhas internas
        if fault_result.affected_zone in ['Z1', 'Z2']:
            diff_operated = any(
                d['should_operate'] and d['type'] == '87T' and fault_result.affected_zone in d['device_id']
                for d in fault_result.device_responses
            )
            if not diff_operated:
                reward -= 15  # Penalidade por diferencial n√£o operar
        
        # === RECOMPENSAS AVAN√áADAS ===
        
        # 11. Margem de coordena√ß√£o adequada
        adequate_margins = sum(1 for issue in fault_result.coordination_issues if issue['margin'] >= 0.25)
        total_margins = len(fault_result.coordination_issues)
        if total_margins > 0:
            margin_ratio = adequate_margins / total_margins
            reward += margin_ratio * 3
        
        # 12. Velocidade de atua√ß√£o apropriada
        fast_devices = [d for d in fault_result.device_responses 
                       if d['should_operate'] and d['operating_time'] < 0.1]
        if len(fast_devices) > 0 and len(fast_devices) <= 2:  # Apenas dispositivos r√°pidos necess√°rios
            reward += 3
        
        return reward
    
    def learn(self, state, action, reward, next_state):
        """Algoritmo Q-Learning avan√ßado para coordena√ß√£o de prote√ß√£o"""
        # Q-Learning com experi√™ncia
        best_next_action = np.argmax(self.q_table[next_state])
        td_target = reward + self.gamma * self.q_table[next_state][best_next_action]
        td_error = td_target - self.q_table[state][action]
        
        # Atualiza√ß√£o com taxa de aprendizado adaptativa
        adaptive_lr = self.learning_rate * (1 + abs(td_error) / 10)  # Maior erro = mais aprendizado
        self.q_table[state][action] += adaptive_lr * td_error
        
        # Decaimento do epsilon apenas ap√≥s explora√ß√£o m√≠nima
        if self.episodes > self.min_exploration_episodes and self.epsilon > self.epsilon_min:
            self.epsilon *= self.epsilon_decay
    
    def get_action(self, state):
        """Sele√ß√£o de a√ß√£o epsilon-greedy melhorada"""
        # Explora√ß√£o for√ßada nas primeiras itera√ß√µes
        if self.episodes < self.min_exploration_episodes:
            return np.random.choice(self.action_space_size)
        
        # Epsilon-greedy padr√£o
        if np.random.random() <= self.epsilon:
            return np.random.choice(self.action_space_size)
        
        # Sele√ß√£o gulosa com ru√≠do para evitar m√≠nimos locais
        q_values = self.q_table[state] + np.random.normal(0, 0.01, self.action_space_size)
        return np.argmax(q_values)
    
    def train_episode(self, coordinator, scenarios):
        """Treina um epis√≥dio com cen√°rios de falta"""
        self.episodes += 1
        episode_reward = 0
        
        for scenario in scenarios:
            # Simula falta
            fault_result = coordinator.simulate_fault(
                scenario['bus'], 
                scenario['fault_type'], 
                scenario['severity']
            )
            
            # Obt√©m estado
            state = self.get_state(coordinator, fault_result)
            
            # Seleciona a√ß√£o
            action = self.get_action(state)
            
            # Aplica a√ß√£o (ajuste nos dispositivos)
            coordinator.apply_rl_action(action)
            
            # Simula novamente ap√≥s ajuste
            new_fault_result = coordinator.simulate_fault(
                scenario['bus'], 
                scenario['fault_type'], 
                scenario['severity']
            )
            
            # Calcula recompensa
            reward = self.calculate_reward(new_fault_result)
            episode_reward += reward
            
            # Obt√©m novo estado
            next_state = self.get_state(coordinator, new_fault_result)
            
            # Aprende
            self.learn(state, action, reward, next_state)
        
        # Decay epsilon
        if self.epsilon > self.epsilon_min:
            self.epsilon *= self.epsilon_decay
        
        # Registra hist√≥rico
        self.training_history.append({
            'episode': self.episodes,
            'reward': episode_reward,
            'epsilon': self.epsilon
        })
        
        return episode_reward

class ProtectionCoordinator:
    """
    Coordenador principal do sistema de prote√ß√£o IEEE 14-Bus
    Com capacidades de simula√ß√£o de faltas e otimiza√ß√£o RL
    """
    
    def __init__(self):
        self.protection_zones = self._create_ieee_14bus_zones()
        self.protection_devices = self._get_all_devices()
        self.rl_agent = BasicRLAgent()
        
        # Imped√¢ncia simplificada do sistema IEEE 14-Bus
        self.bus_impedances = {
            1: 0.1, 2: 0.12, 3: 0.15, 4: 0.08, 5: 0.09, 6: 0.11, 7: 0.13,
            8: 0.14, 9: 0.16, 10: 0.18, 11: 0.19, 12: 0.20, 13: 0.21, 14: 0.22
        }
        
    def _create_ieee_14bus_zones(self):
        """Cria as zonas de prote√ß√£o do sistema IEEE 14-Bus"""
        
                # Configura√ß√£o das Zonas de Prote√ß√£o IEEE 14-Bus - PAR√ÇMETROS NORMATIVOS REAIS
        # Baseado em ABNT NBR 14039, IEEE 242, API RP 14F, IEC 60255-151
        # Zona Z1: Transformador TR1 (25 MVA) - Buses 0,4,5,6,7,9
        zone_z1_devices = [
            ProtectionDevice(
                id="87T-TR1", zone="Z1", type="87T", 
                location="Transformador TR1",
                pickup_current=0.3, time_delay=0.02, distance_km=0.0,  # IEC 60255-8 + API RP 14F
                coordination_margin=0.0  # Instant√¢neo conforme IEEE 242
            ),
            ProtectionDevice(
                id="50/51-L4-5", zone="Z1", type="50/51",
                location="Linha Bus4-Bus5", 
                pickup_current=1.3, time_delay=0.4, distance_km=2.5,  # IEC 60255-151 + IEEE 242
                coordination_margin=0.3  # IEEE C37.112 padr√£o
            ),
            ProtectionDevice(
                id="67-B4", zone="Z1", type="67",
                location="Bus 4",
                pickup_current=1.2, time_delay=0.35, distance_km=0.0,  # IEEE C37.112 + NORSOK E-001
                coordination_margin=0.3
            ),
            ProtectionDevice(
                id="27/59-B7", zone="Z1", type="27/59",
                location="Bus 7",
                pickup_current=0.9, time_delay=1.2, distance_km=3.2,  # ABNT NBR 14039 + API RP 14F
                coordination_margin=0.4  # Maior margem para backup
            )
        ]
        
        # Zona Z2: Transformador TR2 (25 MVA) - Buses 1,5,8,10-14  
        zone_z2_devices = [
            ProtectionDevice(
                id="87T-TR2", zone="Z2", type="87T",
                location="Transformador TR2",
                pickup_current=0.3, time_delay=0.02, distance_km=0.0,  # IEC 60255-8 + API RP 14F
                coordination_margin=0.0  # Instant√¢neo conforme IEEE 242
            ),
            ProtectionDevice(
                id="50/51-L5-6", zone="Z2", type="50/51",
                location="Linha Bus5-Bus6",
                pickup_current=1.4, time_delay=0.5, distance_km=1.8,  # IEC 60255-151 coordenado com Z1
                coordination_margin=0.3  # IEEE C37.112 padr√£o
            ),
            ProtectionDevice(
                id="67-B5", zone="Z2", type="67", 
                location="Bus 5",
                pickup_current=1.3, time_delay=0.45, distance_km=0.0,  # IEEE C37.112 + NORSOK E-001
                coordination_margin=0.3
            ),
            ProtectionDevice(
                id="27/59-B14", zone="Z2", type="27/59",
                location="Bus 14",
                pickup_current=1.0, time_delay=1.4, distance_km=4.1,  # ABNT NBR 14039 + API RP 14F
                coordination_margin=0.4  # Maior margem para backup
            )
        ]
        
        zones = [
            ProtectionZone(
                id="Z1", transformer="TR1 (25 MVA)", power_mva=25, voltage_kv=13.8,
                buses=[0, 4, 5, 6, 7, 9], devices=zone_z1_devices
            ),
            ProtectionZone(
                id="Z2", transformer="TR2 (25 MVA)", power_mva=25, voltage_kv=13.8,
                buses=[1, 5, 8, 10, 11, 12, 13, 14], devices=zone_z2_devices
            )
        ]
        
        return zones
    
    def _get_all_devices(self):
        """Retorna lista de todos os dispositivos"""
        devices = []
        for zone in self.protection_zones:
            devices.extend(zone.devices)
        return devices
    
    def simulate_fault(self, bus: int, fault_type: str, severity: float) -> FaultSimulationResult:
        """
        Simula falta no sistema IEEE 14-Bus
        
        Args:
            bus: N√∫mero da barra (1-14)
            fault_type: Tipo de falta ('3ph', '2ph', '1ph', '2ph_ground')
            severity: Severidade da falta (0.1 a 1.0)
        """
        
        # Determina zona afetada
        affected_zone = "Z1" if bus in [0, 4, 5, 6, 7, 9] else "Z2"
        
        # Calcula corrente de falta usando imped√¢ncia do bus
        base_current = 1000  # A (base)
        bus_impedance = self.bus_impedances.get(bus, 0.15)
        
        # Fatores por tipo de falta
        fault_factors = {'3ph': 1.0, '2ph': 0.87, '1ph': 0.58, '2ph_ground': 0.95}
        fault_factor = fault_factors.get(fault_type, 1.0)
        
        fault_current = (base_current * severity * fault_factor) / bus_impedance
        
        # Simula resposta dos dispositivos
        device_responses = []
        coordination_issues = []
        
        for device in self.protection_devices:
            should_operate = False
            operating_time = float('inf')
            coordination_ok = True
            
            # L√≥gica de opera√ß√£o por tipo de dispositivo
            if device.type == "87T" and device.zone == affected_zone:
                should_operate = fault_current > device.pickup_current * base_current
                operating_time = device.time_delay
            elif device.type == "50/51":
                if fault_current > device.pickup_current * base_current:
                    should_operate = True
                    # Curva tempo-corrente simplificada
                    operating_time = device.time_delay * (1 + 1/(fault_current/base_current - device.pickup_current))
            elif device.type == "67" and device.zone == affected_zone:
                should_operate = fault_current > device.pickup_current * base_current
                operating_time = device.time_delay
            elif device.type == "27/59":
                # Dispositivos de tens√£o operam com atraso
                should_operate = severity > 0.7  # Para faltas severas
                operating_time = device.time_delay
            
            device_responses.append({
                'device_id': device.id,
                'type': device.type,
                'should_operate': should_operate,
                'operating_time': operating_time if should_operate else 0,
                'coordination_ok': coordination_ok
            })
        
        # Verifica coordena√ß√£o entre dispositivos
        operating_devices = [d for d in device_responses if d['should_operate']]
        
        for i, dev1 in enumerate(operating_devices):
            for dev2 in operating_devices[i+1:]:
                time_diff = abs(dev1['operating_time'] - dev2['operating_time'])
                required_margin = 0.3  # IEEE C37.112 padr√£o
                
                if time_diff < required_margin:
                    coordination_issues.append({
                        'device1': dev1['device_id'],
                        'device2': dev2['device_id'],
                        'margin': time_diff,
                        'required': required_margin
                    })
        
        # Avalia conformidade normativa
        normative_compliance = self._evaluate_normative_compliance(
            device_responses, coordination_issues, fault_type
        )
        
        coordination_ok = len(coordination_issues) == 0
        
        return FaultSimulationResult(
            fault_location=f"Bus {bus}",
            fault_type=fault_type,
            fault_current_a=fault_current,
            affected_zone=affected_zone,
            coordination_ok=coordination_ok,
            device_responses=device_responses,
            coordination_issues=coordination_issues,
            normative_compliance=normative_compliance
        )
    
    def _evaluate_normative_compliance(self, device_responses, coordination_issues, fault_type):
        """Avalia conformidade com normas t√©cnicas"""
        
        # IEEE C37.112 - Coordena√ß√£o de prote√ß√£o
        ieee_compliance = len(coordination_issues) == 0
        ieee_issues = [f"Margem insuficiente: {issue['device1']}-{issue['device2']}" 
                      for issue in coordination_issues]
        
        # IEC 61850 - Comunica√ß√£o (simulado)
        iec_compliance = True  # Assumindo GOOSE funcionando
        iec_issues = []
        
        # NBR 5410 - Seletividade
        operating_devices = [d for d in device_responses if d['should_operate']]
        nbr_compliance = len(operating_devices) <= 3  # Seletividade adequada
        nbr_issues = ["Muitos dispositivos operando simultaneamente"] if not nbr_compliance else []
        
        # API RP 14C - Ambiente offshore
        api_compliance = True  # Assumindo equipamentos adequados
        api_issues = []
        
        return {
            'IEEE_C37_112': {'coordination_margins': ieee_compliance, 'issues': ieee_issues},
            'IEC_61850': {'goose_performance': iec_compliance, 'issues': iec_issues},
            'NBR_5410': {'selectivity_dr': nbr_compliance, 'issues': nbr_issues},
            'API_RP_14C': {'offshore_environment': api_compliance, 'issues': api_issues}
        }
    
    def apply_rl_action(self, action):
        """Aplica a√ß√£o avan√ßada do RL mapeando para ajustes espec√≠ficos"""
        # Mapeamento sofisticado de 32 a√ß√µes para 8 dispositivos x 4 ajustes
        device_idx = action // 4  # 8 dispositivos (0-7)
        adjustment_type = action % 4  # 4 tipos de ajuste por dispositivo
        
        if device_idx >= len(self.protection_devices):
            return  # A√ß√£o inv√°lida
        
        device = self.protection_devices[device_idx]
        
        # Ajustes baseados no tipo de a√ß√£o com limites normativos
        if adjustment_type == 0:  # Reduzir pickup (mais sens√≠vel)
            factor = 0.95
            new_pickup = device.pickup_current * factor
            # Limites baseados em IEEE C37.112 e NBR 14039
            if new_pickup >= 50:  # M√≠nimo 50A para coordena√ß√£o
                device.pickup_current = new_pickup
                
        elif adjustment_type == 1:  # Aumentar pickup (menos sens√≠vel)
            factor = 1.05
            new_pickup = device.pickup_current * factor
            # M√°ximo baseado na corrente de carga + margem
            if new_pickup <= 2000:  # M√°ximo 2000A
                device.pickup_current = new_pickup
                
        elif adjustment_type == 2:  # Reduzir tempo (mais r√°pido)
            factor = 0.95
            new_time = device.time_delay * factor
            # Tempo m√≠nimo para estabilidade (IEC 60255-151)
            if new_time >= 0.1:  # M√≠nimo 100ms
                device.time_delay = new_time
                
        elif adjustment_type == 3:  # Aumentar tempo (coordena√ß√£o)
            factor = 1.05
            new_time = device.time_delay * factor
            # Tempo m√°ximo para coordena√ß√£o (IEEE C37.112)
            if new_time <= 2.0:  # M√°ximo 2.0s
                device.time_delay = new_time
        
        # Log de auditoria dos ajustes
        if not hasattr(self, 'adjustment_log'):
            self.adjustment_log = []
        
        self.adjustment_log.append({
            'device': device.id,
            'action_type': adjustment_type,
            'pickup_current': device.pickup_current,
            'time_delay': device.time_delay,
            'timestamp': pd.Timestamp.now().isoformat()
        })
    
    def train_rl_agent(self, episodes=5, scenarios=None):
        """Treina o agente RL com cen√°rios de falta"""
        if scenarios is None:
            scenarios = [
                {'bus': 4, 'fault_type': '3ph', 'severity': 0.8},
                {'bus': 7, 'fault_type': '2ph', 'severity': 0.6},
                {'bus': 14, 'fault_type': '1ph', 'severity': 0.5}
            ]
        
        results = []
        for episode in range(episodes):
            reward = self.rl_agent.train_episode(self, scenarios)
            results.append({
                'episode': episode + 1,
                'reward': reward,
                'epsilon': self.rl_agent.epsilon
            })
        
        return {
            'episodes_completed': episodes,
            'total_episodes': self.rl_agent.episodes,
            'final_epsilon': self.rl_agent.epsilon,
            'results': results
        }

    def optimize_protection_with_rl(self, episodes=100):
        """Otimiza√ß√£o avan√ßada usando RL com m√∫ltiplos cen√°rios"""
        print(f"ü§ñ Iniciando otimiza√ß√£o RL avan√ßada: {episodes} epis√≥dios")
        
        # Cen√°rios de teste diversificados
        fault_scenarios = [
            {'bus': 4, 'fault_type': '3ph', 'severity': 0.9},   # Falta severa
            {'bus': 7, 'fault_type': '2ph', 'severity': 0.7},   # Falta m√©dia
            {'bus': 14, 'fault_type': '1ph', 'severity': 0.5},  # Falta leve
            {'bus': 1, 'fault_type': '3ph', 'severity': 0.8},   # Gera√ß√£o  
            {'bus': 9, 'fault_type': '2ph', 'severity': 0.6},   # Distribui√ß√£o
        ]
        
        optimization_results = []
        best_coordination_score = 0
        best_device_settings = None
        
        for episode in range(episodes):
            episode_rewards = []
            
            # Salvar configura√ß√£o atual
            current_settings = {device.id: (device.pickup_current, device.time_delay) 
                              for device in self.protection_devices}
            
            # M√∫ltiplos passos por epis√≥dio
            for step in range(5):
                # Estado atual
                current_state = self.rl_agent.get_state(self.protection_devices, self.protection_zones)
                
                # A√ß√£o do RL
                action = self.rl_agent.get_action(current_state)
                
                # Aplicar a√ß√£o
                self.apply_rl_action(action)
                
                # Avaliar com todos os cen√°rios
                step_reward = 0
                coordination_quality = 0
                
                for scenario in fault_scenarios:
                    result = self.simulate_fault(
                        scenario['bus'], 
                        scenario['fault_type'], 
                        scenario['severity']
                    )
                    reward = self.rl_agent.calculate_reward(result)
                    step_reward += reward
                    
                    # M√©tricas de qualidade
                    if result.coordination_ok:
                        coordination_quality += 20
                    coordination_quality -= len(result.coordination_issues) * 5
                
                step_reward /= len(fault_scenarios)  # M√©dia dos cen√°rios
                coordination_quality /= len(fault_scenarios)
                episode_rewards.append(step_reward)
                
                # Pr√≥ximo estado
                next_state = self.rl_agent.get_state(self.protection_devices, self.protection_zones)
                
                # Aprendizado
                self.rl_agent.learn(current_state, action, step_reward, next_state)
            
            # Avalia√ß√£o do epis√≥dio
            episode_avg_reward = np.mean(episode_rewards)
            
            # Teste final de coordena√ß√£o
            final_coordination_score = self._evaluate_coordination_quality()
            
            # Salvar melhor configura√ß√£o
            if final_coordination_score > best_coordination_score:
                best_coordination_score = final_coordination_score
                best_device_settings = {device.id: (device.pickup_current, device.time_delay) 
                                      for device in self.protection_devices}
            
            optimization_results.append({
                'episode': episode + 1,
                'avg_reward': episode_avg_reward,
                'coordination_score': final_coordination_score,
                'epsilon': self.rl_agent.epsilon,
                'adjustments_made': len(getattr(self, 'adjustment_log', []))
            })
            
            # Log de progresso
            if episode % 20 == 0 or episode == episodes - 1:
                print(f"üìä Epis√≥dio {episode + 1}: Reward={episode_avg_reward:.2f}, "
                      f"Coordena√ß√£o={final_coordination_score:.1f}, Œµ={self.rl_agent.epsilon:.3f}")
        
        # Aplicar melhor configura√ß√£o encontrada
        if best_device_settings:
            for device in self.protection_devices:
                if device.id in best_device_settings:
                    device.pickup_current, device.time_delay = best_device_settings[device.id]
        
        print(f"‚úÖ Otimiza√ß√£o conclu√≠da! Melhor score: {best_coordination_score:.1f}")
        
        return {
            'episodes_completed': episodes,
            'best_coordination_score': best_coordination_score,
            'final_epsilon': self.rl_agent.epsilon,
            'total_adjustments': len(getattr(self, 'adjustment_log', [])),
            'optimization_history': optimization_results,
            'improvement': best_coordination_score - optimization_results[0]['coordination_score'] if optimization_results else 0
        }
    
    def _evaluate_coordination_quality(self):
        """Avalia qualidade geral do sistema de coordena√ß√£o"""
        total_score = 0
        test_scenarios = [
            {'bus': 4, 'fault_type': '3ph', 'severity': 0.8},
            {'bus': 7, 'fault_type': '2ph', 'severity': 0.6},
            {'bus': 14, 'fault_type': '1ph', 'severity': 0.5}
        ]
        
        for scenario in test_scenarios:
            result = self.simulate_fault(scenario['bus'], scenario['fault_type'], scenario['severity'])
            
            # Pontua√ß√£o baseada em crit√©rios normativos
            if result.coordination_ok:
                total_score += 30
            
            # Penalidades por problemas
            total_score -= len(result.coordination_issues) * 10
            
            # Seletividade (poucos dispositivos operando)
            operating_count = len([d for d in result.device_responses if d['should_operate']])
            if operating_count <= 2:
                total_score += 15  # Boa seletividade
            elif operating_count > 4:
                total_score -= 10  # Seletividade ruim
        
        return max(0, total_score / len(test_scenarios))  # Score normalizado
    
    def get_system_status(self):
        """Retorna status completo do sistema"""
        return {
            'total_devices': len(self.protection_devices),
            'zones': len(self.protection_zones),
            'system_health': 'healthy',
            'rl_agent_status': 'operational' if self.rl_agent.episodes > 0 else 'training'
        }
    
    def get_rl_status(self):
        """Retorna status do agente RL"""
        return {
            'status': 'operational' if self.rl_agent.episodes > 10 else 'training',
            'episodes': self.rl_agent.episodes,
            'max_episodes': self.rl_agent.max_episodes,
            'learning_rate': self.rl_agent.learning_rate,
            'epsilon': self.rl_agent.epsilon,
            'state_size': self.rl_agent.state_size,
            'action_space_size': self.rl_agent.action_space_size
        }

# Teste b√°sico
if __name__ == "__main__":
    print("üîÑ Testando ProtecAI Mini - Sistema RL...")
    coordinator = ProtectionCoordinator()
    print(f"‚úÖ Sistema inicializado com {len(coordinator.protection_devices)} dispositivos")
    
    # Teste de simula√ß√£o
    result = coordinator.simulate_fault(bus=4, fault_type='3ph', severity=0.8)
    print(f"‚úÖ Simula√ß√£o: Zona {result.affected_zone}, Coordena√ß√£o: {result.coordination_ok}")
    print(f"‚úÖ Corrente de falta: {result.fault_current_a:.0f} A")
    
    # Teste do RL
    print("ü§ñ Testando agente RL...")
    train_result = coordinator.train_rl_agent(episodes=3)
    print(f"‚úÖ RL treinado: {train_result['episodes_completed']} epis√≥dios")
    print("‚úÖ Sistema RL FUNCIONAL!")
