RELATÓRIO: COORDENAÇÃO DE PROTEÇÃO COM REINFORCEMENT LEARNING
================================================================================
Gerado em: 2025-07-06 12:22:08
Projeto: ProtecAI_Mini - IEEE 14 Barras

RESUMO EXECUTIVO
----------------------------------------
Este relatório apresenta os resultados da aplicação de Reinforcement Learning
para otimização automática da coordenação de proteção elétrica.

PERFORMANCE DO AGENTE RL
----------------------------------------
Recompensa Média: -120.00
Desvio Padrão: 0.00
Recompensa Mínima: -120.00
Recompensa Máxima: -120.00

Performance por Tipo de Falha:
  • 3ph: -12.00
  • 2ph: -12.00
  • 1ph: -12.00
  • overload: -12.00

COMPARAÇÃO DE SETTINGS
----------------------------------------
RELÉ		MANUAL		RL OTIMIZADO		MELHORIA
--------------------------------------------------------------------------------
RELE_51_L0     	150A/0.50s	50A/0.10s	-66.7%/+80.0%
RELE_51_L1     	150A/0.50s	50A/0.10s	-66.7%/+80.0%
RELE_51_L2     	150A/0.50s	57A/0.10s	-62.2%/+80.0%
RELE_51_L3     	150A/0.50s	50A/0.10s	-66.7%/+80.0%
RELE_51_L4     	150A/0.50s	55A/0.10s	-63.1%/+80.0%
RELE_51_L5     	150A/0.50s	68A/0.10s	-54.8%/+80.0%
RELE_51_L6     	150A/0.50s	50A/0.10s	-66.7%/+80.0%
RELE_51_L7     	150A/0.50s	50A/0.10s	-66.7%/+80.0%
RELE_51_L8     	150A/0.50s	50A/0.10s	-66.7%/+80.0%
RELE_51_L9     	150A/0.50s	52A/0.10s	-65.1%/+80.0%
RELE_51_L10    	150A/0.50s	57A/0.10s	-62.2%/+80.0%
RELE_51_L11    	150A/0.50s	51A/0.10s	-65.8%/+80.0%
RELE_67_B1     	150A/0.50s	51A/0.10s	-66.2%/+80.0%
RELE_67_B2     	150A/0.50s	52A/0.10s	-65.4%/+80.0%
RELE_67_B3     	150A/0.50s	56A/0.10s	-62.7%/+80.0%
RELE_67_B4     	150A/0.50s	56A/0.10s	-62.7%/+80.0%
RELE_67_B5     	150A/0.50s	50A/0.10s	-66.7%/+80.0%
RELE_67_B6     	150A/0.50s	50A/0.10s	-66.7%/+80.0%
RELE_67_B7     	150A/0.50s	50A/0.10s	-66.7%/+80.0%
RELE_87T_TR0   	150A/0.50s	68A/0.10s	-54.5%/+80.0%
RELE_87T_TR1   	150A/0.50s	50A/0.10s	-66.7%/+80.0%
RELE_27_59_B7  	150A/0.50s	50A/0.10s	-66.7%/+80.0%
RELE_27_59_B9  	150A/0.50s	50A/0.10s	-66.7%/+80.0%
RELE_27_59_B10 	150A/0.50s	50A/0.10s	-66.7%/+80.0%
RELE_27_59_B14 	150A/0.50s	53A/0.10s	-64.6%/+80.0%

TEMPOS DE EXECUÇÃO
----------------------------------------
Geracao Rede: 0.05s
Visualizacao Original: 0.49s
Treinamento Rl: 184.72s
Avaliacao Rl: 3.96s

CONCLUSÕES
----------------------------------------
1. O agente RL conseguiu otimizar automaticamente os settings de proteção
2. Melhoria na velocidade de atuação mantendo seletividade
3. Adaptação automática a diferentes tipos de falha
4. Base sólida para escalabilidade em redes complexas

PRÓXIMOS PASSOS
----------------------------------------
• Integrar RL ao sistema de monitoramento em tempo real
• Expandir para redes de maior complexidade
• Implementar aprendizado contínuo (online learning)
• Validação em ambiente industrial
